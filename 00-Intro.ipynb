{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "648d7658",
   "metadata": {},
   "source": [
    "Refrence:https://towardsdatascience.com/natural-language-processing-nlp-for-machine-learning-d44498845d5b\n",
    "# Natural Language Processing:\n",
    "\n",
    "NLP is a field in machine learning with the ability of a computer to understand, analyze, manipulate, and potentially generate human language.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d56f04",
   "metadata": {},
   "source": [
    "## Step1-Reading and Exploring Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f469036f",
   "metadata": {},
   "source": [
    "## Step2-Pre-processing Data\n",
    "Cleaning up the text data is necessary to highlight attributes that we’re going to want our machine learning system to pick up on. Cleaning (or pre-processing) the data typically consists of a number of steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66765644",
   "metadata": {},
   "source": [
    "### 1. Remove punctuation\n",
    "Punctuation can provide grammatical context to a sentence which supports our understanding. But for our vectorizer which counts the number of words and not the context, it does not add value, so we remove all special characters. eg: How are you?->How are you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e285f5",
   "metadata": {},
   "source": [
    "### 2.Tokenization\n",
    "Tokenizing separates text into units such as sentences or words. It gives structure to previously unstructured text. eg: Plata o Plomo-> ‘Plata’,’o’,’Plomo’."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076badb8",
   "metadata": {},
   "source": [
    "### 3. Remove stopwords\n",
    "Stopwords are common words that will likely appear in any text. They don’t tell us much about our data so we remove them. eg: silver or lead is fine for me-> silver, lead, fine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef6d2b6",
   "metadata": {},
   "source": [
    "### Preprocessing Data: Stemming\n",
    "Stemming helps reduce a word to its stem form. It often makes sense to treat related words in the same way. It removes suffices, like “ing”, “ly”, “s”, etc. by a simple rule-based approach. It reduces the corpus of words but often the actual words get neglected. eg: Entitling,Entitled->Entitl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addc817c",
   "metadata": {},
   "source": [
    "### Preprocessing Data: Lemmatizing\n",
    "Lemmatizing derives the canonical form (‘lemma’) of a word. i.e the root form. It is better than stemming as it uses a dictionary-based approach i.e a morphological analysis to the root word.eg: Entitling, Entitled->Entitle\n",
    "In Short, Stemming is typically faster as it simply chops off the end of the word, without understanding the context of the word. Lemmatizing is slower and more accurate as it takes an informed analysis with the context of the word in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1e7408",
   "metadata": {},
   "source": [
    "## Step3-Vectorizing Data\n",
    "Vectorizing is the process of encoding text as integers i.e. numeric form to create feature vectors so that machine learning algorithms can understand our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e4e05d",
   "metadata": {},
   "source": [
    "### Vectorizing Data: Bag-Of-Words\n",
    "Bag of Words (BoW) or CountVectorizer describes the presence of words within the text data. It gives a result of 1 if present in the sentence and 0 if not present. It, therefore, creates a bag of words with a document-matrix count in each text document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4457b9bd",
   "metadata": {},
   "source": [
    "### Vectorizing Data: N-Grams\n",
    "N-grams are simply all combinations of adjacent words or letters of length n that we can find in our source text. Ngrams with n=1 are called unigrams. Similarly, bigrams (n=2), trigrams (n=3) and so on can also be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a805d084",
   "metadata": {},
   "source": [
    "### Vectorizing Data: TF-IDF\n",
    "It computes “relative frequency” that a word appears in a document compared to its frequency across all documents. It is more useful than “term frequency” for identifying “important” words in each document (high frequency in that document, low frequency in other documents).\n",
    "Note: Used for search engine scoring, text summarization, document clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298ab57a",
   "metadata": {},
   "source": [
    "## Step4-Feature Engineering: Feature Creation\n",
    "Feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work. It is like an art as it requires domain knowledge and it can tough to create features, but it can be fruitful for ML algorithm to predict results as they can be related to the prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a187d15e",
   "metadata": {},
   "source": [
    "## Step5-Building ML Classifiers: Model selection\n",
    "We use an ensemble method of machine learning where multiple models are used and their combination produces better results than a single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d547111",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
